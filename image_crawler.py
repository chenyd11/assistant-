#!/usr/bin/env python3
"""
å›¾ç‰‡æœç´¢çˆ¬è™« - ç”¨äº LoRA è®­ç»ƒæ•°æ®æ”¶é›†
æ³¨æ„ï¼šè¯·éµå®ˆç½‘ç«™æœåŠ¡æ¡æ¬¾å’Œç‰ˆæƒæ³•è§„
"""

import requests
from bs4 import BeautifulSoup
import os
import time
import random
from urllib.parse import urljoin, urlparse

class ImageCrawler:
    def __init__(self):
        self.session = requests.Session()
        # ä¼ªè£…æµè§ˆå™¨è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        self.session.headers.update(self.headers)
        
    def search_duckduckgo(self, query, max_results=10):
        """ä» DuckDuckGo æœç´¢å›¾ç‰‡é“¾æ¥"""
        print(f"ğŸ” æœç´¢: {query}")
        
        # æ„é€ æœç´¢ URL
        search_url = f"https://duckduckgo.com/"
        params = {'q': query}
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šè·å– token
            response = self.session.get(search_url, params=params, timeout=10)
            time.sleep(random.uniform(2, 4))  # éšæœºå»¶è¿Ÿ
            
            # å°è¯•ä»é¡µé¢æå–å›¾ç‰‡
            soup = BeautifulSoup(response.text, 'html.parser')
            images = []
            
            # æŸ¥æ‰¾å›¾ç‰‡å…ƒç´ 
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src')
                if src and src.startswith('http'):
                    images.append({
                        'url': src,
                        'alt': img.get('alt', '')
                    })
                    if len(images) >= max_results:
                        break
            
            print(f"âœ… æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡")
            return images
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_bing(self, query, max_results=10):
        """ä» Bing æœç´¢å›¾ç‰‡é“¾æ¥ï¼ˆæ›´å‹å¥½ï¼‰"""
        print(f"ğŸ” Bing æœç´¢: {query}")
        
        search_url = "https://www.bing.com/images/search"
        params = {'q': query}
        
        try:
            response = self.session.get(search_url, params=params, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            images = []
            # Bing å›¾ç‰‡é€šå¸¸åœ¨ murl å±æ€§ä¸­
            for a in soup.find_all('a', class_='iusc'):
                m = a.get('m')
                if m:
                    import json
                    try:
                        data = json.loads(m)
                        img_url = data.get('murl')
                        if img_url:
                            images.append({
                                'url': img_url,
                                'alt': data.get('desc', '')
                            })
                            if len(images) >= max_results:
                                break
                    except:
                        pass
            
            print(f"âœ… Bing æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡")
            return images
            
        except Exception as e:
            print(f"âŒ Bing æœç´¢å¤±è´¥: {e}")
            return []
    
    def download_image(self, url, output_dir, filename):
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            # éšæœºå»¶è¿Ÿï¼Œé¿å…è§¦å‘é™åˆ¶
            time.sleep(random.uniform(1, 3))
            
            response = self.session.get(url, timeout=15, stream=True)
            
            if response.status_code == 200:
                # ç¡®å®šæ–‡ä»¶æ‰©å±•å
                content_type = response.headers.get('content-type', '')
                if 'jpeg' in content_type or 'jpg' in content_type:
                    ext = '.jpg'
                elif 'png' in content_type:
                    ext = '.png'
                elif 'webp' in content_type:
                    ext = '.webp'
                else:
                    ext = '.jpg'
                
                filepath = os.path.join(output_dir, f"{filename}{ext}")
                
                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                print(f"âœ… ä¸‹è½½æˆåŠŸ: {filepath}")
                return filepath
            else:
                print(f"âŒ HTTP {response.status_code}: {url}")
                return None
                
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {url} - {e}")
            return None
    
    def crawl_images(self, query, output_dir, max_images=10):
        """å®Œæ•´çš„çˆ¬å–æµç¨‹"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸš€ å¼€å§‹çˆ¬å–: {query}")
        print(f"ğŸ“ ä¿å­˜åˆ°: {output_dir}")
        print("-" * 50)
        
        # å°è¯•å¤šä¸ªæœç´¢å¼•æ“
        all_images = []
        
        # Bingï¼ˆæ›´å‹å¥½ï¼‰
        bing_images = self.search_bing(query, max_results=max_images)
        all_images.extend(bing_images)
        
        if len(all_images) < max_images:
            # DuckDuckGo
            ddg_images = self.search_duckduckgo(query, max_results=max_images - len(all_images))
            all_images.extend(ddg_images)
        
        # å»é‡
        seen_urls = set()
        unique_images = []
        for img in all_images:
            if img['url'] not in seen_urls:
                seen_urls.add(img['url'])
                unique_images.append(img)
        
        print(f"\nğŸ“Š å…±æ‰¾åˆ° {len(unique_images)} å¼ å”¯ä¸€å›¾ç‰‡")
        print("-" * 50)
        
        # ä¸‹è½½å›¾ç‰‡
        downloaded = []
        for i, img in enumerate(unique_images[:max_images], 1):
            print(f"\n[{i}/{min(len(unique_images), max_images)}] ä¸‹è½½: {img['url'][:60]}...")
            filepath = self.download_image(img['url'], output_dir, f"image_{i:03d}")
            if filepath:
                downloaded.append(filepath)
        
        print("\n" + "=" * 50)
        print(f"ğŸ‰ å®Œæˆï¼æˆåŠŸä¸‹è½½ {len(downloaded)}/{max_images} å¼ å›¾ç‰‡")
        print(f"ğŸ“‚ ä¿å­˜ä½ç½®: {output_dir}")
        
        return downloaded


def main():
    """ä¸»å‡½æ•°"""
    # æœç´¢å…³é”®è¯
    query = "Android 18 Dragon Ball"
    
    # è¾“å‡ºç›®å½•
    output_dir = "/Users/chenyd11/Desktop/android18_crawl"
    
    # æœ€å¤§ä¸‹è½½æ•°é‡
    max_images = 10
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = ImageCrawler()
    
    # å¼€å§‹çˆ¬å–
    downloaded = crawler.crawl_images(query, output_dir, max_images)
    
    return downloaded


if __name__ == "__main__":
    main()
